import enum
import time

from typing import List, Optional, Tuple, Dict, Union, Iterable

from .policy import PolicyFactory
from .wrappers import Request, RequestStatus, SchedulerOutput
from sduss.config import SchedulerConfig
from sduss.logger import init_logger
from sduss.utils import Counter
from sduss.worker import WorkerOutput

logger = init_logger(__name__)

class PreemptionMode(enum.Enum):
    """Preemption Modes

    Attributes:
        SWAP: Swap out the blocks of the preempted sequences to CPU memory
            and swap the back in when the sequences are resumed.
        RECOMPUTE: Discard the blocks of the preempted sequences and recompute
            them when the sequences are resumed, treating the sequences as
            new prompts.
    """
    SWAP = enum.auto()
    RECOMPUTE  = enum.auto()


# req_id -> req
RequestQueue = Dict[int, Request]
StateQueue = Tuple[List[Request], int, RequestStatus]
        
class Scheduler:
    """Main scheduler which arranges tasks.
    
    Attributes:
        prompt_limit: Length limit of the prompt derived from configuration.
    """
    
    def __init__(
        self,
        scheduler_config: SchedulerConfig,
    ) -> None:
        self.scheduler_config = scheduler_config

        # Unpack scheduler config's argumnents
        self.max_batchsize = scheduler_config.max_batchsize
        
        # Scheduler policy
        self.policy = PolicyFactory.get_policy(policy_name="fcfs")

        # resolution -> queues
        # queue name -> RequestQueue
        self.request_pool: Dict[int, Dict[str, RequestQueue]] = {}
        # resolution -> number of unfinished reqs
        self.num_unfinished_reqs: Dict[int, int] = {}
        
        # Lazy import to avoid circular import
        from sduss.scheduler import SUPPORT_RESOLUTION
        for res in SUPPORT_RESOLUTION:
            self._initialize_resolution_queues(res)
        
        # Set status mapping: name -> (priority, status)
        # Priority here only reflects the order of process stage
        self.status_mapping = {
            "waiting": (10, RequestStatus.WAITING),
            "prepare": (20, RequestStatus.PREPARE),
            "denoising": (30, RequestStatus.DENOISING),
            "postprocessing": (40, RequestStatus.POSTPROCESSING),
        }


    def _initialize_resolution_queues(self, res: int) -> None:
        res_queues: Dict[str, RequestQueue] = {}
        res_queues["waiting"] = {}
        res_queues["prepare"] = {}
        res_queues["denoising"] = {}
        res_queues["postprocessing"] = {}
        self.request_pool[res] = res_queues
        self.num_unfinished_reqs[res] = 0

        
    def add_request(self, req: Request) -> None:
        """Add a new request to waiting queue."""
        res = req.sampling_params.resolution
        self.request_pool[res]["waiting"][req.request_id] = req
        self.num_unfinished_reqs[res] += 1

        
    def abort_request(self, request_ids: Union[int, Iterable[int]]) -> None:
        """Abort a handful of requests.

        Args:
            request_ids (Union[str, Iterable[str]]): Requests to be aborted.
        """        
        # TODO
        pass

    
    def has_unfinished_requests(self) -> bool:
        for num in self.num_unfinished_reqs.values():
            if num > 0:
                return True
        return False 


    def get_num_unfinished_requests(self) -> int:
        total = 0
        for num in self.num_unfinished_reqs.values():
            total += num
        return total

    
    def _schedule(self) -> SchedulerOutput:
        """Schedules running and swapping operations.

        Returns:
            SchedulerOutputs: All information generated by scheduling.
        """        

        # Fix the current time
        now = time.monotonic()
        
        # Decide which stage for this iteration
        target_status = self._decide_stage()
        target_queue = self._get_queue_from_status(target_status)
        
        cur_batchsize = 0
        collected_reqs: List[Request] = []
        sorted_queue = self.policy.sort_by_priority(now, target_queue)
        for req in sorted_queue:
            # TODO(MX): Currently we cannot handle any single request that
            # contains num_imgs > self.max_batchsize.
            req_size = req.sampling_params.num_imgs
            if req_size + cur_batchsize > self.max_batchsize:
                break

            # TODO(MX): Prepare stage reqs don't need to be compatible, since they are executed separately.
            if len(collected_reqs) > 0:
                # Only add compatible targets
                if collected_reqs[0].sampling_params.is_compatible_with(req.sampling_params):
                    collected_reqs.append(req)
                    cur_batchsize += req_size
            else:
                collected_reqs.append(req)
                cur_batchsize += req_size

        scheduler_outputs = SchedulerOutput(
            scheduled_requests=collected_reqs,
            status=target_status,
        )
        return scheduler_outputs
                
            
    def schedule(self) -> SchedulerOutput:
        """Schedule requests for next iteration."""
        scheduler_output = self._schedule()
        # More wrappers will be added here.
        
        return scheduler_output
    
    
    def update_reqs_status(
        self,
        scheduler_outputs: SchedulerOutput,
        request_ids: List[int],
        output: WorkerOutput,
    ):
        """Update requests after one iteration."""
        # Move this req to next queue
        sche_status = scheduler_outputs.status
        sche_reqs = scheduler_outputs.scheduled_requests
        next_status = self._get_next_status(sche_status)
        if sche_status == RequestStatus.WAITING:
            self._update_reqs_to_status_queue(prev_status=sche_status, status=next_status, reqs=sche_reqs)
            return 
        elif sche_status == RequestStatus.PREPARE:
            # First iteration of denoising has done
            self._update_reqs_to_status_queue(prev_status=sche_status, status=next_status, reqs=sche_reqs)
            # Some reqs might only iterate once
            denoising_complete_reqs = self._decrease_one_step(sche_reqs)
            if len(denoising_complete_reqs) > 0:
                self._update_reqs_to_status_queue(prev_status=next_status, 
                                                  status=RequestStatus.POSTPROCESSING, 
                                                  reqs=denoising_complete_reqs)
            return 
        elif sche_status == RequestStatus.DENOISING:
            # More steps done
            # may or may not move to post stage
            denoising_complete_reqs = self._decrease_one_step(sche_reqs)
            if len(denoising_complete_reqs) > 0:
                self._update_reqs_to_status_queue(prev_status=next_status, 
                                                  status=RequestStatus.POSTPROCESSING, 
                                                  reqs=denoising_complete_reqs)
            return 
        elif sche_status == RequestStatus.POSTPROCESSING:
            assert output is not None
            # engine is responsible ofr prepare output, we don't need to do so here
            # free finished requests automatically
            self._free_finished_requests(sche_reqs)
            
        
    def _free_finished_requests(self, reqs: List[Request]) -> None:
        """Untrack all finished requests."""
        for req in reqs:
            assert req.status == RequestStatus.POSTPROCESSING
            self.postprocess.remove(req)
        
        for req in reqs:
            
        
        
    def _get_queue_from_name(self, name: str) -> List[Request]:
        if name not in self.state_queue.keys():
            raise RuntimeError("Invalid queue name was requested.")
        return self.state_queue[name][0]

    
    def _get_queue_from_status(self, status: RequestStatus) -> List[Request]:
        if status == RequestStatus.WAITING:
            return self.waiting
        elif status == RequestStatus.PREPARE:
            return self.prepare
        elif status == RequestStatus.DENOISING:
            return self.state_queue
        elif status == RequestStatus.POSTPROCESSING:
            return self.postprocess
        
        
    def _decide_stage(self) -> RequestStatus:
        target_status = self.policy.decide_stage(self.state_queue)
        if target_status is None:
            raise RuntimeError("No queue in scheduler has remaining requsts to process.")
        return target_status

    
    def _get_next_status(self, prev_status: RequestStatus) -> RequestStatus:
        if prev_status == RequestStatus.WAITING:
            return RequestStatus.PREPARE
        elif prev_status == RequestStatus.PREPARE:
            return RequestStatus.DENOISING
        elif prev_status == RequestStatus.DENOISING:
            return RequestStatus.POSTPROCESSING
        elif prev_status == RequestStatus.POSTPROCESSING:
            return RequestStatus.FINISHED_STOPPED
        else:
            raise ValueError(f"Not next status available for {prev_status}")
    
    
    def _update_reqs_to_status_queue(self, prev_status: RequestStatus, status: RequestStatus, reqs: List[Request]):
        prev_queue = self._get_queue_from_status(prev_status)
        target_queue = self._get_queue_from_status(status)
        for req in reqs:
            prev_queue.remove(req)
            req.status = status
            target_queue.append(req)
 

    def _decrease_one_step(self, reqs: List[Request]) -> List[Request]:
        denoising_complete_reqs: List[Request] = []
        for req in reqs:
            # Prepare stage has been updated to denoising, it's safe to do so
            assert req.status == RequestStatus.DENOISING
            req.remain_steps -= 1
            if req.remain_steps == 0:
                denoising_complete_reqs.append(req)
        return denoising_complete_reqs